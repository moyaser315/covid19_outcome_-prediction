{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mohamed yaser elsaid 2001226\n",
    "# yassa siefen ayed 2001307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "firstly we will explore the data and take in considerations columns we nedd and drop others we don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### it appears that the unamed is indexing so we will drop it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(\"Unnamed: 0\",axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from describe there are some unusual beahviours observed as :\n",
    "- the min in the diff coulmn is -ve value\n",
    "- symptom 6 has its percentiles and max equal to each other\n",
    "- gender have a max of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fristly we explore the diff as it's described in the project statement that it's the time the symptoms appeared so it can't be less than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['diff_sym_hos'] <0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next we explore the values of sympotom 6 and the gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['symptom6'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### as can be seen there's only 1 zero so the symptom doesn't effect any model so we will drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = df['gender'].value_counts()\n",
    "print(gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### an undefined variable 2  exists with big frequency so that makes it redundant to make sure we will pie plot the percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[1,0,2] # as one appeared before 0 in the data\n",
    "plt.pie(gender,labels=labels,startangle=180,autopct=\"%2.2f%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can drop symptom 6 as it doesn't affect the data and gender can be dropped but as instructed that the data is clean so we will keep it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['result' ,'symptom6'],axis = 1).values #['gender' ,'result','symptom6']\n",
    "y = df['result'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting into train , validate and test\n",
    "- random_state is chosen 50 for recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train ,X_test,y_train,y_test = train_test_split(X,y,shuffle=True,test_size=0.2,random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feautre scaling\n",
    " - after testing each model will be scalled indvidually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing metrics and validations to be used as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay , auc\n",
    "from sklearn.metrics import roc_curve , RocCurveDisplay\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "score_idx = [\"acc\",\"recall\",\"percision\",\"f1 score\",\"AUC\"]\n",
    "comp={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(y_pred,y_true=y_test):\n",
    "    res = []\n",
    "    res +=[accuracy_score(y_true=y_true,y_pred=y_pred)]\n",
    "    res += [recall_score(y_true=y_true,y_pred=y_pred)]\n",
    "    res += [precision_score(y_true=y_true,y_pred=y_pred)]\n",
    "    res += [f1_score(y_true=y_true,y_pred=y_pred)]\n",
    "    print(f\"accuracy score :{res[0]:0.3f} \")\n",
    "    print(f\"recall score :{res[1]:0.3f}     percesion score : {res[2]:0.3f}\")\n",
    "    print(f\"f1 score :{res[-1]:0.3f} \")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confmat(y_pred,y_true=y_test):\n",
    "    cm=confusion_matrix(y_pred=y_pred,y_true=y_true)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocgraph(y_pred,name,y_test=y_test):\n",
    "    fpr, tpr, _=roc_curve(y_test,y_pred)\n",
    "    roc=auc(fpr,tpr)\n",
    "    display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc,estimator_name=name)\n",
    "    display.plot()\n",
    "    return (roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO discover the problem ---- > inconsisitent\n",
    "def kf_validation(model,scoring,n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits,shuffle=True,random_state=50)\n",
    "    best_model = None\n",
    "    best_rec = 0.0\n",
    "    methods = {\"acc\":accuracy_score,\n",
    "               \"rec\":recall_score,\n",
    "               \"pre\":precision_score}\n",
    "    score = methods[scoring]\n",
    "    records=[]\n",
    "    for tr_idx ,test_idx in kf.split(X_train) :\n",
    "\n",
    "        ## splitting to train and validation\n",
    "        xtr,xte=X_train[tr_idx],X_train[test_idx]\n",
    "        ytr,yte=y_train[tr_idx],y_train[test_idx]\n",
    "\n",
    "        #training\n",
    "        classifier_k = model\n",
    "        print(score)\n",
    "        classifier_k.fit(xtr,ytr)\n",
    "\n",
    "        #testing\n",
    "        y_hat = classifier_k.predict(xte)\n",
    "\n",
    "        #recording and choosing best model\n",
    "        records+=[score(y_pred=y_hat,y_true=yte)]\n",
    "        if records[-1] > best_rec :\n",
    "            best_rec = records[-1]\n",
    "            best_model = classifier_k\n",
    "        \n",
    "    print(f\"best {scoring} :{best_rec} \")\n",
    "    print(f\"avg {scoring} : {np.mean(records)}\")\n",
    "    return best_model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after searching we found we can do dimensionality reduction to draw a decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary(model) :\n",
    "    pca = PCA(n_components=2)\n",
    "    X_test_pca = pca.fit_transform(X_test)\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X_test_pca[:, 0].min() - 1, X_test_pca[:, 0].max() + 1\n",
    "    y_min, y_max = X_test_pca[:, 1].min() - 1, X_test_pca[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain predictions for each point in the meshgrid\n",
    "    Z = model.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "\n",
    "    # Reshape the predictions for contour plotting\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "\n",
    "    # Scatter plot of training data points\n",
    "    plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, edgecolors='k', cmap=plt.cm.RdYlBu)\n",
    "\n",
    "    plt.title('Decision Boundary Visualization using PCA')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logestic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logestic regression reached max iteration with kfolds for validation and without, so we apply standrixation for feautres  and will use grid search to look for best options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_log = sc.fit_transform(X_train)\n",
    "X_test_log = sc.transform(X_test)\n",
    "X_val_log = sc.transform(X_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logestic regression with accuracy as its scoring value without any validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifierLR = LogisticRegression()\n",
    "classifierLR.fit(X_train_log,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_LR = classifierLR.predict(X_test_log)\n",
    "confmat(y_LR)\n",
    "LR_scores = scores(y_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= rocgraph(y_LR,'logistic regression')\n",
    "LR_scores+=[a]\n",
    "comp['logistic regression']=LR_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary(classifierLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_LR = classifierLR.predict(X_val_log)\n",
    "confmat(y_LR,y_validate)\n",
    "scores(y_LR,y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using k-folds cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10,shuffle=True,random_state=50)\n",
    "best_model = None\n",
    "best_rec = 0.0\n",
    "recs=[]\n",
    "for tr_idx ,test_idx in kf.split(X_train_log) :\n",
    "\n",
    "    xtr,xte=X_train_log[tr_idx],X_train_log[test_idx]\n",
    "    ytr,yte=y_train[tr_idx],y_train[test_idx]\n",
    "\n",
    "    classifier_k = LogisticRegression()\n",
    "    classifier_k.fit(xtr,ytr)\n",
    "    y_hat = classifier_k.predict(xte)\n",
    "    recs+=[recall_score(y_pred=y_hat,y_true=yte)]\n",
    "    if recs[-1] > best_rec :\n",
    "        best_rec = recs[-1]\n",
    "        best_model = classifier_k\n",
    "print(\"best recall : \", best_rec)\n",
    "print(\"avg rec : \", np.mean(recs))\n",
    "best_model.fit(X_train_log, y_train)\n",
    "y_hat = best_model.predict(X_test_log)\n",
    "confmat(y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rocgraph(y_hat,'logistic regression')\n",
    "KFold_LR_scores = scores(y_hat)\n",
    "KFold_LR_scores += [a]\n",
    "comp['logistic regression k']=KFold_LR_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_LR = classifierLR.predict(X_val_log)\n",
    "confmat(y_LR,y_validate)\n",
    "scores(y_LR,y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using GridSearch to find the best paramters to pass into the object creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for params we will use : \n",
    " - C : the defintion in they API is :\n",
    " > Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization \n",
    "  - we already used standrization so we will check if it needs to be multiblied by a number so we will start at 1 to 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [1,10,100,1000,10000],\n",
    "}\n",
    "best_model = LogisticRegression()\n",
    "grid_search = GridSearchCV(best_model, param_grid, verbose = 1, cv=10,scoring='recall')\n",
    "\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train_log, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = list(grid_search.best_params_.values())\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=LogisticRegression(C=best_params[0])\n",
    "best_model.fit(X_train_log, y_train)\n",
    "y_hat = best_model.predict(X_test_log)\n",
    "confmat(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rocgraph(y_hat,'logistic regression')\n",
    "KFold_LR_scores = scores(y_hat)\n",
    "KFold_LR_scores += [a]\n",
    "comp['logistic regression grid']=KFold_LR_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_LR = classifierLR.predict(X_val_log)\n",
    "confmat(y_LR,y_true=y_validate)\n",
    "LR_scores = scores(y_LR,y_validate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame.from_dict(comp)\n",
    "res_df.index = score_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid search is much better after trading percision for recall giving a higher AUC and F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
